{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G8ImbWL0Vhtz"
   },
   "source": [
    "# finBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "egmUZz8LVW6L"
   },
   "source": [
    "## Import Key Developments News dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jHIj1si5VSIW"
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy as np\n",
    "\n",
    "headlines_df = pandas.read_csv('AAPL 2022-2023.csv')\n",
    "headlines_df = headlines_df[headlines_df['headline'].notna()]\n",
    "headlines_df = headlines_df[headlines_df['headline'] != '']\n",
    "\n",
    "headlines_array = np.array(headlines_df)\n",
    "\n",
    "headlines_list = list(headlines_array[:,4])\n",
    "stocks_list = list(headlines_array[:, 2])\n",
    "date_list = list(headlines_array[:, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jBXjZiTUV1c2"
   },
   "source": [
    "## Getting the tokenizer and the finBERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Id63ND7eV5Ns"
   },
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "  \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"ProsusAI/finbert\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LgKTM0kHWD9b"
   },
   "source": [
    "## Performing inference on the stock market news headlines with the finBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ccr1w4UXWGuf"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def chunk_list(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "\n",
    "\n",
    "STRIDE = 50\n",
    "\n",
    "model.eval()\n",
    "\n",
    "sentiment_table = pandas.DataFrame(columns=['headline', 'stock', 'pos', 'neg', 'neutr', 'date'])\n",
    "\n",
    "n=0\n",
    "for lines, stocks, dates in zip(chunk_list(headlines_list, STRIDE), chunk_list(stocks_list, STRIDE), \n",
    "                                chunk_list(date_list, STRIDE)):\n",
    "  \n",
    "  input = tokenizer(lines, padding = True, truncation = True,  return_tensors='pt')\n",
    "  \n",
    "  outputs = model(**input)\n",
    "\n",
    "  prediction = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "\n",
    "  print(f\"{n+1}/{int(len(headlines_list)/STRIDE)}\") \n",
    "\n",
    "  for headline, stock, pos, neg, neutr, date in zip(lines, stocks, prediction[:, 0].tolist(), \n",
    "                                                    prediction[:, 1].tolist(), prediction[:, 2].tolist(), dates): \n",
    "    sentiment_table.loc[len(sentiment_table)] = {'headline': headline, 'stock': stock, 'pos': pos, 'neg': neg, \n",
    "                                                 'neutr': neutr, 'date':date}\n",
    "   \n",
    "  n+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lnh_hJJRWLeM"
   },
   "source": [
    "## Save the sentiment dataframe as a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fORYoKQOWQRg"
   },
   "outputs": [],
   "source": [
    "sentiment_table = pandas.DataFrame(sentiment_table)\n",
    "print(sentiment_table)\n",
    "\n",
    "sentiment_table.to_csv('AAPL_sentiment_table.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cSBmy234tWFU"
   },
   "outputs": [],
   "source": [
    "# Group by date and calculate average for pos, neg, neutr columns and take first value for stock column\n",
    "sent_for_training = sentiment_table.groupby('date').agg({'stock': 'first', 'pos': 'mean', 'neg': 'mean', \n",
    "                                                         'neutr': 'mean'})\n",
    "\n",
    "# Sort by date (index) in ascending order\n",
    "sent_for_training.sort_index(ascending=True, inplace=True)\n",
    "# Save the new DataFrame to a csv file\n",
    "sent_for_training.to_csv('AAPL_2022_2023_FOR_TRAINING.csv', index=False)\n",
    "\n",
    "print(sent_for_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XIRtcq7XWlX7"
   },
   "source": [
    "# Processing the 2 datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HVN6PSUzWsKm"
   },
   "source": [
    "## utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "49HZq2-VBnFq"
   },
   "outputs": [],
   "source": [
    "!pip install pandas-datareader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UPKO6f3SWo1k"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas_datareader import data as data_reader\n",
    "import yfinance as yf\n",
    "\n",
    "yf.pdr_override()\n",
    "\n",
    "from tqdm import tqdm_notebook, tqdm\n",
    "from collections import deque\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Re5uOKkWvvB"
   },
   "source": [
    "## Loading the two datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ILrMJi2JWy9T"
   },
   "outputs": [],
   "source": [
    "stock_name = 'AAPL'\n",
    "start_date = '2023-1-1'\n",
    "end_date = '2023-2-28'\n",
    "\n",
    "date_range = pd.date_range(start=start_date, end=end_date)\n",
    "\n",
    "dataset = data_reader.get_data_yahoo(stock_name, start=start_date, end=end_date)\n",
    "sentiment_table = pd.read_csv(stock_name + '_sentiment_table.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PwKCsbVZW31u"
   },
   "source": [
    "## Create TRAINING dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XKneApbXW8Bs"
   },
   "outputs": [],
   "source": [
    "# Add 'virality' feature which counts the number of news itmes there are per day\n",
    "sentiment_table['virality'] = sentiment_table.groupby('date')['date'].transform('size')\n",
    "\n",
    "# Average sentiment per day\n",
    "sent_for_training = sentiment_table.groupby('date').agg({'stock': 'first', 'pos': 'mean', 'neg': 'mean', \n",
    "                                                         'neutr': 'mean', 'virality': 'first'})\n",
    "\n",
    "\n",
    "sent_for_training.sort_index(ascending=True, inplace=True)\n",
    "merged_df = dataset.copy()\n",
    "merged_df['pos'] = np.nan\n",
    "merged_df['neg'] = np.nan\n",
    "merged_df['neutr'] = np.nan\n",
    "merged_df['virality'] = np.nan\n",
    "\n",
    "# add sentiment from days when market is open and create the closed days dataset of sentiment\n",
    "closed_df = pd.DataFrame()\n",
    "closed_rows =[]\n",
    "for date in sent_for_training.index:\n",
    "  # Check if the date exists in merged_df\n",
    "    if date in merged_df.index:\n",
    "      # Copy 'pos', 'neg', 'neutr' values from sent_for_training to merged_df\n",
    "        merged_df.loc[date, 'pos'] = sent_for_training.loc[date, 'pos']\n",
    "        merged_df.loc[date, 'neg'] = sent_for_training.loc[date, 'neg']\n",
    "        merged_df.loc[date, 'neutr'] = sent_for_training.loc[date, 'neutr']\n",
    "        merged_df.loc[date, 'virality'] = sent_for_training.loc[date, 'virality']\n",
    "    else:\n",
    "        # If the date is not in merged_df, add it to 'closed' DataFrame\n",
    "        closed_row = sent_for_training.loc[[date]]\n",
    "        closed_rows.append(closed_row)\n",
    "if len(closed_rows) > 0:\n",
    "  closed_df = pd.concat(closed_rows)\n",
    "\n",
    "# add sentiment from news when market was closed\n",
    "counting = False\n",
    "closed_days = []\n",
    "\n",
    "for date in date_range:\n",
    "    date_str = date.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    if counting and date_str in merged_df.index: \n",
    "        if pd.isna(merged_df.loc[date_str, 'pos']):\n",
    "            merged_df.loc[date_str, ['pos', 'neg', 'neutr', 'virality']] = \n",
    "            closed_df.loc[closed_days].mean(numeric_only=True)\n",
    "        else:\n",
    "            merged_df.loc[date_str, ['pos', 'neg', 'neutr', 'virality']] += \n",
    "            closed_df.loc[closed_days].mean(numeric_only=True)\n",
    "        closed_df = closed_df.drop(closed_days)\n",
    "        closed_days = []\n",
    "        counting = False\n",
    "        \n",
    "    if date_str in closed_df.index:\n",
    "        counting = True\n",
    "        closed_days.append(date_str)\n",
    "\n",
    "merged_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# set to baseline 0 sentiment if first day has no news\n",
    "merged_df.iloc[0] = merged_df.iloc[0].fillna(0)\n",
    "# set the sentiment to previous values on days when no news was released\n",
    "merged_df['pos'].fillna(method='ffill', inplace=True)\n",
    "merged_df['neg'].fillna(method='ffill', inplace=True)\n",
    "merged_df['neutr'].fillna(method='ffill', inplace=True)\n",
    "merged_df['virality'].fillna(method='ffill', inplace=True)\n",
    "\n",
    "#normalise the data\n",
    "scaler = MinMaxScaler()\n",
    "# Scale \"Open\", \"High\", \"Low\", \"Close\", \"Adj Close\" columns together\n",
    "merged_df[['Open', 'High', 'Low', 'Close', 'Adj Close']] = scaler.fit_transform(merged_df[['Open', 'High', \n",
    "                                                                                'Low', 'Close', 'Adj Close']])\n",
    "# Scale \"Volume\", \"pos\", \"neg\", \"neutr\" columns individually\n",
    "for col in ['Volume', 'virality']:\n",
    "    merged_df[[col]] = scaler.fit_transform(merged_df[[col]])\n",
    "\n",
    "print(merged_df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4zzS4_hHX80X"
   },
   "source": [
    "## Save training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0YeNbI4YX_JR"
   },
   "outputs": [],
   "source": [
    "merged_df.to_csv(stock_name + '_2022_2023_Training.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8NSjEA2AYL2D"
   },
   "source": [
    "# S-DRQN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kNhbDjSJWbSH"
   },
   "source": [
    "## Installing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a1CSpxPWWhwI"
   },
   "outputs": [],
   "source": [
    "!pip install pandas-datareader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TLqHNkZDYli2"
   },
   "source": [
    "## utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "75DDDuIMYnA3"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas_datareader import data as data_reader\n",
    "\n",
    "from tqdm import tqdm_notebook, tqdm\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vV1-INOAYrY0"
   },
   "source": [
    "## S-DRQN algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IzQQoeM2T1fS"
   },
   "source": [
    "### AI Day Trader class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UFBDlzYlYvjt"
   },
   "outputs": [],
   "source": [
    "class AIDayTrader:\n",
    "\n",
    "    def __init__(self, state_shape, action_space=3):  # Stay, Buy, Sell\n",
    "\n",
    "        self.state_shape = state_shape\n",
    "        self.action_space = action_space\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.inventory = []\n",
    "        self.model_name = \"AI_Day_Trader\"\n",
    "\n",
    "        self.gamma = 0.99\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_final = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "\n",
    "        # Exponential Decay of Learning Rate\n",
    "        lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=0.0005, \n",
    "                                                                     decay_steps=250, decay_rate=0.99)\n",
    "\n",
    "        self.lr = 0.00025\n",
    "        self.model = self.model_builder()\n",
    "\n",
    "        self.profit = 0\n",
    "        self.spent = 0\n",
    "        self.cumulative_reward = 0\n",
    "        self.last_action_was_buy = False\n",
    "\n",
    "    def model_builder(self):\n",
    "\n",
    "        model = tf.keras.models.Sequential()\n",
    "        model.add(tf.keras.layers.Dense(256, activation='relu', input_shape=[self.state_shape]))\n",
    "        model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
    "        model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
    "        model.add(tf.keras.layers.Dense(units=self.action_space, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=self.lr), \n",
    "                      metrics=['accuracy', 'mae'])\n",
    "\n",
    "        return model\n",
    "\n",
    "    def trade(self, state):\n",
    "        state = np.reshape(state, [1, self.state_shape])\n",
    "\n",
    "        if random.random() <= self.epsilon:\n",
    "            if self.last_action_was_buy:\n",
    "                return random.choice([0, 2])  # Force a hold or sell if the last action was a buy\n",
    "            if len(self.inventory) == 0:\n",
    "                return random.choice([0, 1])  # Force a hold or buy if inventory is empty\n",
    "            else:\n",
    "                return random.randrange(self.action_space)\n",
    "\n",
    "        actions = self.model.predict(state)[0]\n",
    "        action = np.argmax(actions)\n",
    "\n",
    "        if self.last_action_was_buy and action == 1:\n",
    "            sorted_indices = np.argsort(actions)[::-1]\n",
    "            action = sorted_indices[1]  # Force a hold or sell if the last action was a buy\n",
    "        if len(self.inventory) == 0 and action == 2:\n",
    "            sorted_indices = np.argsort(actions)[::-1]\n",
    "            action = sorted_indices[1]  # Force a hold or buy if inventory is empty\n",
    "\n",
    "        return action\n",
    "\n",
    "    def batch_train(self, batch_size):\n",
    "        batch = []\n",
    "\n",
    "        for i in range(len(self.memory) - batch_size + 1, len(self.memory)):\n",
    "            batch.append(self.memory[i])\n",
    "\n",
    "        for state, action, reward, next_state, done in batch:\n",
    "            re_next = np.reshape(next_state, [1, self.state_shape])\n",
    "            re_state = np.reshape(state, [1, self.state_shape])\n",
    "\n",
    "            t_reward = reward\n",
    "\n",
    "            if not done:\n",
    "                t_reward = reward + self.gamma * np.amax(self.model.predict(re_next)[0])\n",
    "\n",
    "            target = self.model.predict(re_state)\n",
    "            target[0, action] = t_reward\n",
    "\n",
    "            self.model.fit(re_state, target, epochs=1, verbose=1)\n",
    "\n",
    "        if self.epsilon > self.epsilon_final:\n",
    "            self.epsilon *= self.epsilon_decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nFBzZ4AanAaE"
   },
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t0U1KBBInDpY"
   },
   "outputs": [],
   "source": [
    "def stocks_price_format(n):\n",
    "    if n < 0:\n",
    "        return \"- $ {0:2f}\".format(abs(n))\n",
    "    else:\n",
    "        return \"$ {0:2f}\".format(abs(n))\n",
    "\n",
    "\n",
    "def dataset_loader(stock_name, start='2022-1-1', end='2023-1-1'):\n",
    "    dataset = data_reader.get_data_yahoo(stock_name, start=start, end=end)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def state_creator(data, timestep):\n",
    "    state = data.iloc[timestep]\n",
    "    return np.array(state)  # Return as a 2D array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "atnn_fDGTs77"
   },
   "source": [
    "### Hyperparameters..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q8U3Ev3pSw6f"
   },
   "outputs": [],
   "source": [
    "# ticker\n",
    "stock_name = \"AAPL\"\n",
    "\n",
    "data = pd.read_csv('AAPL_2022_2023_Training.csv')\n",
    "data = data.drop(data.columns[0], axis=1)\n",
    "\n",
    "episodes = 2500\n",
    "batch_size = 16\n",
    "\n",
    "data_samples = len(data)\n",
    "state_shape = (len(data.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rv-FeWLkTntK"
   },
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BXL8prHGTb1k"
   },
   "outputs": [],
   "source": [
    "trader = AIDayTrader(state_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4zRbzV3fd2DC"
   },
   "outputs": [],
   "source": [
    "trader.model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NUgJd58KTeO9"
   },
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bwuvkZM5SlUV"
   },
   "outputs": [],
   "source": [
    "%%capture cap\n",
    "\n",
    "\n",
    "all_profit = []\n",
    "all_spent = []\n",
    "\n",
    "\n",
    "for episode in range(1, episodes + 1):\n",
    "    print(\"Episode: {}/{}\".format(episode, episodes))\n",
    "\n",
    "    state = state_creator(data, 0)\n",
    "\n",
    "    trader.inventory = []\n",
    "\n",
    "    # Keep track of whether a buy or a sell action was made\n",
    "    buy_action = False\n",
    "    sell_action = False\n",
    "    trader.last_action_was_buy = False\n",
    "\n",
    "    reward_t1 = 0\n",
    "\n",
    "    for t in tqdm(range(data_samples-1)):\n",
    "        print(\"Day: {}/{}\".format(t+1, data_samples-1))\n",
    "\n",
    "        action = trader.trade(state)\n",
    "\n",
    "        next_state = state_creator(data, t+1)\n",
    "\n",
    "        reward_t = 0  # Staying\n",
    "\n",
    "        if action == 1:  # Buying\n",
    "            if not trader.last_action_was_buy:  # Only allow buying if the last action was not a buy\n",
    "                trader.inventory.append(data.iloc[t])\n",
    "                trader.spent += data['Open'][t]\n",
    "                reward_t = data['Close'][t] - data['Open'][t]\n",
    "                print(\"AI Trader bought: \", stocks_price_format(data['Open'][t]))\n",
    "                trader.last_action_was_buy = True\n",
    "                buy_action = True\n",
    "\n",
    "        if action == 2 and len(trader.inventory) > 0:  # Selling\n",
    "            buy_price = trader.inventory.pop(0)\n",
    "            profit = data['Open'][t] - buy_price['Open']\n",
    "            reward_t = -(data['Close'][t] - data['Open'][t])\n",
    "            trader.profit += reward_t + profit\n",
    "            print(\"AI Trader sold: \", stocks_price_format(data['Open'][t]), \" Profit: \" + \n",
    "                  stocks_price_format(reward_t))\n",
    "            trader.last_action_was_buy = False\n",
    "            sell_action = True\n",
    "\n",
    "        if action == 0:  # Staying\n",
    "            if len(trader.inventory) > 0:\n",
    "                reward_t = data['Close'][t] - data['Open'][t]\n",
    "            else:\n",
    "                reward_t = -(data['Close'][t] - data['Open'][t])\n",
    "\n",
    "        if t == data_samples - 2:\n",
    "            done = True\n",
    "            if len(trader.inventory) > 0:\n",
    "                buy_price = trader.inventory.pop(0)\n",
    "                reward_t += -(data['Close'][t] - data['Open'][t])\n",
    "                trader.profit += reward_t\n",
    "                print(\"AI Trader sold: \", stocks_price_format(data['Close'][t]), \" Profit: \" + \n",
    "                      stocks_price_format(reward_t))\n",
    "            if (buy_action == False) or (sell_action == False):\n",
    "                reward_t = -1\n",
    "        else:\n",
    "            done = False\n",
    "\n",
    "        reward_t += reward_t1\n",
    "\n",
    "        if (reward_t*reward_t1 > 0) and (reward_t1 != 0):\n",
    "            trader.cumulative_reward = np.log(reward_t / reward_t1)\n",
    "        else:\n",
    "            trader.cumulative_reward = -1\n",
    "\n",
    "        reward_t1 = reward_t\n",
    "\n",
    "        trader.memory.append((state, action, trader.cumulative_reward, next_state, done))\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "\n",
    "            print(\"########################\")\n",
    "            print(\"TOTAL PROFIT: {}\".format(trader.profit))\n",
    "            print(\"TOTAL SPENT: {}\".format(trader.spent))\n",
    "            print(\"########################\")\n",
    "\n",
    "            all_profit.append(trader.profit)\n",
    "            all_spent.append(trader.spent)\n",
    "\n",
    "            trader.profit = 0\n",
    "            trader.spent = 0\n",
    "\n",
    "            df = pd.DataFrame(all_profit, columns=[\"Total_Profit\"])\n",
    "            df.to_csv(\"total_profit.csv\", index=False)\n",
    "            fd = pd.DataFrame(all_spent, columns=[\"Total_Spending\"])\n",
    "            fd.to_csv(\"total_spent.csv\", index=False)\n",
    "\n",
    "            if len(trader.memory) > batch_size:\n",
    "                trader.batch_train(batch_size)\n",
    "\n",
    "    if episode % 100 == 0:\n",
    "        trader.model.save(\"ai_day_trader_{}.h5\".format(episode))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I2MeCQsr74l5"
   },
   "source": [
    "### Save output shell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OxT29KXb73T4"
   },
   "outputs": [],
   "source": [
    "f = open(\"financial DRQN verbose.txt\", \"w\") \n",
    "print(cap, file=f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m8kZ0uABpJ6A"
   },
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "91o5jYlrpN4c"
   },
   "source": [
    "### Plot profit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fh8Q5VrKpNPj"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('total_profit.csv')\n",
    "data = np.array(data)\n",
    "\n",
    "total = list(data[:,0])\n",
    "\n",
    "# Calculate moving average\n",
    "window_size = 100\n",
    "moving_average = np.convolve(total, np.ones(window_size) / window_size, mode='valid')\n",
    "\n",
    "# Plotting\n",
    "plt.plot(range(len(total)), total, marker='o', label='Actual Values')\n",
    "plt.plot(range(window_size - 1, len(total)), moving_average, label='Moving Average')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Profit')\n",
    "plt.title('Profit per episode')\n",
    "plt.legend()\n",
    "plt.ylim(-2, 1)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ng4WN1_lpsRy"
   },
   "source": [
    "### Plot % return on investment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "twa8mXYepxtZ"
   },
   "outputs": [],
   "source": [
    "profit_data = pd.read_csv('total_profit.csv')\n",
    "spent_data = pd.read_csv('total_spent.csv')\n",
    "\n",
    "# Convert to numpy arrays\n",
    "profit = np.array(profit_data['Total_Profit'])\n",
    "spent = np.array(spent_data['Total_Spending'])\n",
    "\n",
    "# Calculate percentage profit\n",
    "percentage_profit = np.empty(len(profit))\n",
    "for i in range(len(profit)):\n",
    "    if (profit[i] != 0) and (spent[i] != 0):\n",
    "        percentage_profit[i] = np.round((profit[i] / spent[i]) * 100, 3)\n",
    "    else:\n",
    "        percentage_profit[i] = 0\n",
    "\n",
    "# Calculate moving average\n",
    "window_size = 100\n",
    "moving_average = np.convolve(percentage_profit, np.ones(window_size) / window_size, mode='valid')\n",
    "\n",
    "# Plotting\n",
    "plt.plot(range(len(percentage_profit)), percentage_profit, marker='o', label='Actual Values')\n",
    "plt.plot(range(window_size - 1, len(percentage_profit)), moving_average, label='Moving Average')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Profit (%)')\n",
    "plt.title('ROI')\n",
    "plt.legend()\n",
    "plt.ylim(-30, 30)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kMyfq_RrrYMP"
   },
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hDyHb05I6p7R"
   },
   "source": [
    "## util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gc-E8MrC6tOM"
   },
   "outputs": [],
   "source": [
    "!pip install pandas-datareader\n",
    "\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas_datareader import data as data_reader\n",
    "\n",
    "from tqdm import tqdm_notebook, tqdm\n",
    "from collections import deque\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r0geBZV4rZ6q"
   },
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SKr896Pprf1d"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "path = 'ai_day_trader_1500.h5'\n",
    "model = load_model(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cTfx3eP9r2XQ"
   },
   "source": [
    "## Load test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0-e41y_ar6BW"
   },
   "outputs": [],
   "source": [
    "stock_name = 'AAPL'\n",
    "test_df = pd.read_csv(stock_name + '_test_data.csv')\n",
    "test_df = test_df.drop(test_df.columns[0], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vQhvMos8z10g"
   },
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z0VIPBz-z4ZK"
   },
   "outputs": [],
   "source": [
    "test_data = np.array(test_df)\n",
    "\n",
    "predictions = model.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UNSFrjCp2i6O"
   },
   "outputs": [],
   "source": [
    "np.savetxt('Predictions.csv', predictions, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "13RlyjXo0q2E"
   },
   "source": [
    "## Calculate Profit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VWl5dfVW00_B"
   },
   "outputs": [],
   "source": [
    "spent = 0\n",
    "profit = 0\n",
    "reward = 0\n",
    "inventory = []\n",
    "last_action_was_buy = False\n",
    "\n",
    "for i in range (len(predictions)-1):\n",
    "  action = np.argmax(predictions[i])\n",
    "  print(action)\n",
    "  action_taken = False\n",
    "  bought = False\n",
    "\n",
    "  while not action_taken:\n",
    "\n",
    "    if action == 1:  # Buying\n",
    "      if not last_action_was_buy:  # Only allow buying if the last action was not a buy\n",
    "        inventory.append(test_df.iloc[i])\n",
    "        spent += test_df['Open'][i]\n",
    "        print(\"Day: {}/{}\".format(i+1, len(predictions)), \"AI Trader bought: \", \n",
    "              stocks_price_format(test_df['Open'][i]))\n",
    "        last_action_was_buy = True\n",
    "        action_taken = True\n",
    "      else:\n",
    "        sorted_indices = np.argsort(predictions[i])[::-1]\n",
    "        action = sorted_indices[1]\n",
    "      \n",
    "    if action == 2:\n",
    "      if len(inventory) > 0:  # Selling\n",
    "        buy_price = inventory.pop(0)\n",
    "        profit += test_df['Open'][i] - buy_price['Open']\n",
    "        print(\"Day: {}/{}\".format(i+1, len(predictions)), \"AI Trader sold: \", \n",
    "              stocks_price_format(test_df['Open'][i]), \" Profit: \" + stocks_price_format(profit))\n",
    "        last_action_was_buy = False\n",
    "        action_taken = True\n",
    "      else:\n",
    "        sorted_indices = np.argsort(predictions[i])[::-1]\n",
    "        action = sorted_indices[1]\n",
    "\n",
    "    if action == 0:\n",
    "      action_taken = True\n",
    "\n",
    "\n",
    "print('Spent', spent)\n",
    "print('Profit', profit)\n",
    "print('ROI', np.round((profit / spent) * 100, 3))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "G8ImbWL0Vhtz",
    "egmUZz8LVW6L",
    "jBXjZiTUV1c2",
    "LgKTM0kHWD9b",
    "Lnh_hJJRWLeM",
    "HVN6PSUzWsKm",
    "5Re5uOKkWvvB",
    "PwKCsbVZW31u",
    "4zzS4_hHX80X",
    "kNhbDjSJWbSH",
    "TLqHNkZDYli2",
    "99_uK9hTIjcg"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
